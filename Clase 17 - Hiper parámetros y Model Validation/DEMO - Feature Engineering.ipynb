{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos estudiamos las ideas fundamentales de machine learning, pero todos los ejemplos vistos asumieron que teníamos datos numéricos en un formato [n_samples, m_features].\n",
    "\n",
    "En aplicaciones reales, los datos rara vez vienen ordenados de esa manera. Con esto en mente, uno de los pasos más importantes para usar machine learning en la práctica es la **ingeniería de features o feature engineering**, esto es, tomar cualquier información que exista sobre el problema a resolver y convertirla en números que puedan usarse para construir una matriz de features.\n",
    "\n",
    "Normalmente este proceso es conocido como **vectorización**, ya que involucra la idea de convertir datos arbitrarios en vectores bien formados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajando con features categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginemos que estamos explorando ciertos datos de precios de propiedades y, junto a features numéricas como el precio y el total de habitaciones, también tenemos información sobre el barrio en que se encuentra cada propiedad. Por ejemplo, los datos podrían verse así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{'precio': 850000, 'habitaciones': 4, 'barrio': 'Palermo'},\n",
    "        {'precio': 700000, 'habitaciones': 3, 'barrio': 'San Telmo'},\n",
    "        {'precio': 650000, 'habitaciones': 3, 'barrio': 'Villa Luro'},\n",
    "        {'precio': 600000, 'habitaciones': 2, 'barrio': 'La Boca'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la técnica **one-hot encoding**, que crea columnas extra indicando la presencia o ausencia de una categoría con un valor de 1 o 0, respectivamente. Cuando los datos vienen como una lista de diccionarios, la clase **DictVectorizer** hace esto automáticamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vec = DictVectorizer(sparse = False, dtype = int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con las features categóricas codificadas de esta manera, podemos proceder como de costumbre para ajustar un modelo con Scikit-Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver el significado de cada columna, podemos inspeccionar los nombres de las features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay una clara desventaja en esta aproximación: si cada categoría tiene muchos valores posibles, esto puede incrementar en gran medida el tamaño del dataset. Sin embargo, como los datos codificados contienen principalmente ceros, una **representación dispersa (sparse matrix)** podría ser una solución eficiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer(sparse = True, dtype = int)\n",
    "vec.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que la matriz dispersa almacena **únicamente los valores distintos de cero**: en nuestro caso, de un total de 24 elementos (4 x 6), sólo almacenó 12. Esto hace mucho más eficiente el procesamiento. Muchos (aunque no todos) de los estimadores de Scikit-Learn aceptan representaciones dispersas cuando se ajustan y evalúan los modelos.\n",
    "\n",
    "Alternativamente, **sklearn.preprocessing.OneHotEncoder y sklearn.feature_extraction.FeatureHasher** son dos funcionalidades adicionales que Scikit-Learn incluye para soportar este tipo de codificación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajando con texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra necesidad común en feature engineering es convertir texto a un conjunto de valores numéricos representativos. Por ejemplo, la mayoría del mining automático de datos de redes sociales se basa en alguna forma de codificación del texto como números. Uno de los métodos más simples es codificar los datos por medio del  conteo de palabras (*word counts*), que consiste en tomar cada fragmento de texto, contar las ocurrencias de cada palabra en él y volcar los resultados en una tabla.\n",
    "\n",
    "Por ejemplo, consideremos el siguiente dataset de tres frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = ['científico de datos',\n",
    "          'datos estructurados',\n",
    "          'pensamiento científico']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para vectorizar este dataset basado en el conteo de palabras, podríamos construir una columna representando cada palabra: \"científico\", \"datos\", \"pensamiento\", etc.\n",
    "\n",
    "Para esto usamos **CountVectorizer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(textos)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar el output del CountVectorizer utilizando el método `todense()`, que convierte la **matrix dispersa a una matriz densa**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos volcar este resultado en un `DataFrame`. Para obtener el encabezado de las columnas, vamos a utilizar el método `get_feature_names()`, propio del vectorizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data = X.todense(), columns = vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term frequency-inverse document frequency (TF–IDF)** es una técnica alternativa que computa la frecuencia relativa de cada palabra por documento, ponderada por la inversa de su frecuencia relativa a lo largo del *corpus* (colección de documentos). Este método funciona mejor con ciertos algoritmos de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer()\n",
    "X = vec.fit_transform(textos)\n",
    "pd.DataFrame(data = X.todense(), columns = vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features derivadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideremos el siguiente dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([4, 2, 1, 3, 7])\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset claramente no puede ser descrito correctamente por una línea recta. Sin embargo, podemos ajustar una línea a los datos usando `LinearRegression` y obtener el siguiente resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = x[:, np.newaxis]\n",
    "model = LinearRegression().fit(X, y)\n",
    "yfit = model.predict(X)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder explicar correctamente estos datos, necesitaríamos un modelo más sofisticado que describa la relación entre x e y. \n",
    "\n",
    "Una aproximación a esto es transformar los datos, agregando columnas extra de features para agregar más flexibilidad al modelo. Por ejemplo, podemos agregar **features polinómicas** a los datos de esta forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(degree = 3, include_bias = False)\n",
    "X2 = poly.fit_transform(X)\n",
    "print(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matriz de features derivada tiene una columna representando x, una segunda columna representando x al cuadrado, y una tercer columna representando x al cubo. Computar una regresión lineal en esta entrada expandida nos da un ajuste mucho más cercano a nuestros datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X2, y)\n",
    "yfit = model.predict(X2)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea de mejorar una solución, no cambiando el modelo, pero tranformando la entrada, es fundamental para muchos de los métodos más poderosos de machine learning.   \n",
    "\n",
    "Más generalmente, éeste es un tema que motiva la creación de las técnicas conocidas como kernel methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajando con datos faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra necesidad común en feature engineering es el manejo de datos faltantes. Por ejemplo, podríamos tener un dataset como este:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "\n",
    "X = np.array([[ nan, 0,   3  ],\n",
    "              [ 3,   7,   9  ],\n",
    "              [ 3,   5,   2  ],\n",
    "              [ 4,   nan, 6  ],\n",
    "              [ 8,   8,   1  ]])\n",
    "\n",
    "y = np.array([14, 16, -1,  8, -5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aplicar un modelo de machine learning a estos datos, primero debemos reemplazar los datos faltantes con algún valor apropiado. Esto se conoce como **imputación de datos faltantes** y las estrategias van desde las simples (reemplazar valores faltantes con la media de la columna) hasta las más sofisticadas (como usar modelos robustos para imputación).\n",
    "\n",
    "Como un ejemplo simple de imputación, usaremos la media. Scikit-Learn provee la clase **Imputer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(strategy = 'mean')\n",
    "X2 = imp.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos armar una secuencia de transformaciones, puede ser tedioso hacerlo a mano. Por ejemplo, podríamos querer hacer algo como esto:\n",
    "\n",
    "1. Imputar valores perdidos usando la media\n",
    "2. Transformar features a cuadráticas\n",
    "3. Ajustar una regresión lineal\n",
    "\n",
    "Para organizar este tipo de pipeline de procesamiento, Scikit-Learn provee la clase **Pipeline**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(Imputer(strategy = 'mean'),\n",
    "                      PolynomialFeatures(degree = 2),\n",
    "                      LinearRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este **Pipeline** se comporta como un objeto estándar de Scikit-Learn, y **aplicará todos los pasos especificados a cualquier dato de entrada**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)  # X contiene valores faltantes, pero el primer paso del Pipeline es imputarles la media\n",
    "print(y)\n",
    "print(model.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
